{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9153efd-55c2-4aff-8773-b886fdddb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# --- Parameters (must match your training pipeline) ---\n",
    "desired_time_seconds = 4.0\n",
    "sample_rate = 22050  # This is the default from librosa.load()\n",
    "num_mel_bins = 128\n",
    "class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d03d94bc-3f04-4a17-9fa9-1007c2201f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebd-marco\\Documents\\homeaudio_ml\\homeaudio\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 1: Load the TFLite model ---\n",
    "interpreter = tf.lite.Interpreter(model_path='C:\\\\Users\\\\ebd-marco\\\\Documents\\\\homeaudio_ml\\\\notebooks\\\\sound_classifier_quantized.tflite')\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9b754f2-2da8-40fe-bafc-b27645484c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Interpreter expects input shape: [  1 128 173   1]\n",
      "DEBUG: Interpreter expects input dtype: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_shape = input_details[0]['shape']\n",
    "\n",
    "# DEBUG PRINT: What does the interpreter expect?\n",
    "print(f\"DEBUG: Interpreter expects input shape: {input_details[0]['shape']}\")\n",
    "print(f\"DEBUG: Interpreter expects input dtype: {input_details[0]['dtype']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7af3b4c-8ca8-4515-93ec-93a1cedef1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 2: Load and preprocess a test audio file ---\n",
    "# Replace 'your_audio_file.wav' with the path to a file you want to test\n",
    "test_audio_path = 'C:\\\\Users\\\\ebd-marco\\\\Documents\\\\homeaudio_ml\\\\notebooks\\\\your_audio_file.mp3' \n",
    "\n",
    "if not os.path.exists(test_audio_path):\n",
    "    print(f\"Error: The audio file '{test_audio_path}' does not exist.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2720318-cedf-4d02-b528-1b75107ad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the audio and resample to the correct rate\n",
    "audio, _ = librosa.load(test_audio_path, sr=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d31299-bfd9-418b-b2b3-0da040ee11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad or truncate the audio to the desired length\n",
    "desired_samples = int(desired_time_seconds * sample_rate)\n",
    "if len(audio) < desired_samples:\n",
    "    samples_to_pad = desired_samples - len(audio)\n",
    "    formatted_audio = np.pad(audio, (0, samples_to_pad), 'constant')\n",
    "else:\n",
    "    formatted_audio = audio[:desired_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14742fff-a10c-45c9-a417-5ec77afca7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Prepared input data shape: (1, 128, 173, 1)\n",
      "DEBUG: Prepared input data dtype: float32\n",
      "DEBUG: Prepared input data min/max values: -80.0 / 3.814697265625e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the Mel spectrogram\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=formatted_audio, sr=sample_rate, n_mels=num_mel_bins)\n",
    "\n",
    "# Convert to decibel scale\n",
    "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "# Add batch and channel dimensions to match the model's input shape\n",
    "input_data = mel_spectrogram_db[np.newaxis, ..., np.newaxis].astype(np.float32)\n",
    "\n",
    "# DEBUG PRINT: What does the prepared data look like?\n",
    "print(f\"\\nDEBUG: Prepared input data shape: {input_data.shape}\")\n",
    "print(f\"DEBUG: Prepared input data dtype: {input_data.dtype}\")\n",
    "print(f\"DEBUG: Prepared input data min/max values: {input_data.min()} / {input_data.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb49fddf-3d82-4fc7-b02e-a0263323fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 3: Run Inference ---\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4edda19-a844-48cf-b625-5366a2e71727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Raw output tensor shape: (1, 10)\n",
      "DEBUG: Raw output tensor dtype: float32\n",
      "DEBUG: Raw output tensor values: [[0.1015625 0.1015625 0.1015625 0.1015625 0.1015625 0.1015625 0.1015625\n",
      "  0.1015625 0.1015625 0.1015625]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 4: Get and interpret the output ---\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_class_id = np.argmax(output_data)\n",
    "predicted_class_name = class_names[predicted_class_id]\n",
    "\n",
    "# DEBUG PRINT: What did the output tensor look like before interpretation?\n",
    "print(f\"\\nDEBUG: Raw output tensor shape: {output_data.shape}\")\n",
    "print(f\"DEBUG: Raw output tensor dtype: {output_data.dtype}\")\n",
    "print(f\"DEBUG: Raw output tensor values: {output_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "516db001-1692-420e-89b1-d207e252eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class ID: 0\n",
      "Predicted class name: air_conditioner\n",
      "Model output probabilities: [[0.1015625 0.1015625 0.1015625 0.1015625 0.1015625 0.1015625 0.1015625\n",
      "  0.1015625 0.1015625 0.1015625]]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "print(f\"Predicted class name: {predicted_class_name}\")\n",
    "print(f\"Model output probabilities: {output_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc1378-19cd-42ae-bbb0-7b2bdce2d018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
